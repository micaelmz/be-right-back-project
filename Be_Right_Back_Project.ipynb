{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Tentativa 1</h1>\n",
        "<h2>Chatterbot</h2>"
      ],
      "metadata": {
        "id": "nvxJ3R-UUBa8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ee7tJxO-CwlW",
        "outputId": "144e05ad-e91d-4386-ff32-eb3710da587f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting chatterbot\n",
            "  Using cached ChatterBot-1.0.5-py2.py3-none-any.whl (67 kB)\n",
            "Collecting sqlalchemy<1.3,>=1.2\n",
            "  Downloading SQLAlchemy-1.2.19.tar.gz (5.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.7 MB 5.0 MB/s \n",
            "\u001b[?25hCollecting pymongo<4.0,>=3.3\n",
            "  Downloading pymongo-3.13.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (526 kB)\n",
            "\u001b[K     |████████████████████████████████| 526 kB 56.3 MB/s \n",
            "\u001b[?25hCollecting spacy<2.2,>=2.1\n",
            "  Using cached spacy-2.1.9.tar.gz (30.7 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.8/dist-packages (from chatterbot) (2022.6)\n",
            "Collecting pint>=0.8.1\n",
            "  Using cached Pint-0.20.1-py3-none-any.whl (269 kB)\n",
            "Requirement already satisfied: nltk<4.0,>=3.2 in /usr/local/lib/python3.8/dist-packages (from chatterbot) (3.7)\n",
            "Collecting python-dateutil<2.8,>=2.7\n",
            "  Using cached python_dateutil-2.7.5-py2.py3-none-any.whl (225 kB)\n",
            "Collecting pyyaml<5.2,>=5.1\n",
            "  Using cached PyYAML-5.1.2.tar.gz (265 kB)\n",
            "Collecting mathparse<0.2,>=0.1\n",
            "  Using cached mathparse-0.1.2-py3-none-any.whl (7.2 kB)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk<4.0,>=3.2->chatterbot) (2022.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk<4.0,>=3.2->chatterbot) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk<4.0,>=3.2->chatterbot) (1.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk<4.0,>=3.2->chatterbot) (4.64.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil<2.8,>=2.7->chatterbot) (1.15.0)\n",
            "Collecting plac<1.0.0,>=0.9.6\n",
            "  Using cached plac-0.9.6-py2.py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<2.2,>=2.1->chatterbot) (1.0.9)\n",
            "Collecting srsly<1.1.0,>=0.0.6\n",
            "  Using cached srsly-1.0.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (211 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<2.2,>=2.1->chatterbot) (2.23.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<2.2,>=2.1->chatterbot) (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<2.2,>=2.1->chatterbot) (1.21.6)\n",
            "Collecting blis<0.3.0,>=0.2.2\n",
            "  Using cached blis-0.2.4-cp38-cp38-linux_x86_64.whl\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<2.2,>=2.1->chatterbot) (2.0.7)\n",
            "Collecting preshed<2.1.0,>=2.0.1\n",
            "  Using cached preshed-2.0.1-cp38-cp38-linux_x86_64.whl\n",
            "Collecting thinc<7.1.0,>=7.0.8\n",
            "  Using cached thinc-7.0.8-cp38-cp38-linux_x86_64.whl\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.2,>=2.1->chatterbot) (2022.12.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.2,>=2.1->chatterbot) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.2,>=2.1->chatterbot) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.2,>=2.1->chatterbot) (1.24.3)\n",
            "Building wheels for collected packages: pyyaml, spacy, sqlalchemy\n",
            "  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyyaml: filename=PyYAML-5.1.2-cp38-cp38-linux_x86_64.whl size=44117 sha256=da650777c2e72e440663c4d7d0b8b4ad39c7b136e54ebed66d3b6b491d06c3a4\n",
            "  Stored in directory: /root/.cache/pip/wheels/55/40/9f/027c3d94280ce2b7c2c107cb563a433e6572f830a5462231ae\n",
            "  Building wheel for spacy (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for spacy: filename=spacy-2.1.9-cp38-cp38-linux_x86_64.whl size=43514050 sha256=966eeaed3092c44ae231b31e5aa822a06f815c8b62930d231ebcd6d3797f3640\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/c2/73/989e659e83b30a74acdfcb825e632b1a7dc12f39d58fe37dd6\n",
            "  Building wheel for sqlalchemy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlalchemy: filename=SQLAlchemy-1.2.19-cp38-cp38-linux_x86_64.whl size=1155379 sha256=bd5d32239cd1e82db71196abd104ff7ba161944d968d67b8cbd34923683f5a4d\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/13/f8/47c2f3157957c3693caffa64a94a718cb1357fe186e4d52e48\n",
            "Successfully built pyyaml spacy sqlalchemy\n",
            "Installing collected packages: srsly, preshed, plac, blis, thinc, sqlalchemy, spacy, pyyaml, python-dateutil, pymongo, pint, mathparse, chatterbot\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 2.4.5\n",
            "    Uninstalling srsly-2.4.5:\n",
            "      Successfully uninstalled srsly-2.4.5\n",
            "  Attempting uninstall: preshed\n",
            "    Found existing installation: preshed 3.0.8\n",
            "    Uninstalling preshed-3.0.8:\n",
            "      Successfully uninstalled preshed-3.0.8\n",
            "  Attempting uninstall: blis\n",
            "    Found existing installation: blis 0.7.9\n",
            "    Uninstalling blis-0.7.9:\n",
            "      Successfully uninstalled blis-0.7.9\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.1.5\n",
            "    Uninstalling thinc-8.1.5:\n",
            "      Successfully uninstalled thinc-8.1.5\n",
            "  Attempting uninstall: sqlalchemy\n",
            "    Found existing installation: SQLAlchemy 1.4.45\n",
            "    Uninstalling SQLAlchemy-1.4.45:\n",
            "      Successfully uninstalled SQLAlchemy-1.4.45\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.4.4\n",
            "    Uninstalling spacy-3.4.4:\n",
            "      Successfully uninstalled spacy-3.4.4\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 6.0\n",
            "    Uninstalling PyYAML-6.0:\n",
            "      Successfully uninstalled PyYAML-6.0\n",
            "  Attempting uninstall: python-dateutil\n",
            "    Found existing installation: python-dateutil 2.8.2\n",
            "    Uninstalling python-dateutil-2.8.2:\n",
            "      Successfully uninstalled python-dateutil-2.8.2\n",
            "  Attempting uninstall: pymongo\n",
            "    Found existing installation: pymongo 4.3.3\n",
            "    Uninstalling pymongo-4.3.3:\n",
            "      Successfully uninstalled pymongo-4.3.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "prophet 1.1.1 requires python-dateutil>=2.8.0, but you have python-dateutil 2.7.5 which is incompatible.\n",
            "en-core-web-sm 3.4.1 requires spacy<3.5.0,>=3.4.0, but you have spacy 2.1.9 which is incompatible.\n",
            "dask 2022.2.1 requires pyyaml>=5.3.1, but you have pyyaml 5.1.2 which is incompatible.\n",
            "confection 0.0.3 requires srsly<3.0.0,>=2.4.0, but you have srsly 1.0.6 which is incompatible.\u001b[0m\n",
            "Successfully installed blis-0.2.4 chatterbot-1.0.5 mathparse-0.1.2 pint-0.20.1 plac-0.9.6 preshed-2.0.1 pymongo-3.13.0 python-dateutil-2.7.5 pyyaml-5.1.2 spacy-2.1.9 sqlalchemy-1.2.19 srsly-1.0.6 thinc-7.0.8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "dateutil"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install chatterbot"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from chatterbot import ChatBot\n",
        "from chatterbot.trainers import ChatterBotCorpusTrainer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "CQbKtze5DBic",
        "outputId": "87244150-50c4-405f-cabc-55b15b8b7246"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-e342df816dde>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mchatterbot\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChatBot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchatterbot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChatterBotCorpusTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/chatterbot/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mChatterBot\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mmachine\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconversational\u001b[0m \u001b[0mdialog\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \"\"\"\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mchatterbot\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChatBot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'1.0.5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/chatterbot/chatterbot.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mchatterbot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStorageAdapter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchatterbot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogic\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogicAdapter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchatterbot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIndexedTextSearch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchatterbot\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/chatterbot/storage/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mchatterbot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage_adapter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStorageAdapter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchatterbot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdjango_storage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDjangoStorageAdapter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchatterbot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmongodb\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMongoDatabaseAdapter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchatterbot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_storage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSQLStorageAdapter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/chatterbot/storage/storage_adapter.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchatterbot\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlanguages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mchatterbot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagging\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPosHypernymTagger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/chatterbot/tagging.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchatterbot\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlanguages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchatterbot\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mchatterbot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_sentence_tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpos_tag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/chatterbot/tokenizers.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunkt\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPunktSentenceTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPunktTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mchatterbot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_corpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist_corpus_files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchatterbot\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlanguages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/chatterbot/corpus.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0myaml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mchatterbot_corpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDATA_DIRECTORY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'chatterbot_corpus'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Crie um chatbot e defina o treinador\n",
        "chatbot = ChatBot(\"My Chatbot\")\n",
        "trainer = ChatterBotCorpusTrainer(chatbot)"
      ],
      "metadata": {
        "id": "oDCu_m-XC750"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Treine o chatbot com os diálogos fornecidos\n",
        "trainer.train(\"chatterbot.corpus.english.greetings\",\n",
        "              \"chatterbot.corpus.english.conversations\")"
      ],
      "metadata": {
        "id": "Dz-60XRNC_RF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Agora você pode usar o chatbot para gerar respostas automatizadas\n",
        "response = chatbot.get_response(\"oi\")\n",
        "print(response)\n",
        "\n",
        "response = chatbot.get_response(\"como esta\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "fhmc9PPRC4pm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<i><h3>Tentativa 1 - Falhou</h3>\n",
        "Motivo: a biblioteca chatterbot está depreciada, não recebe atualizaçòes e nào foi possivel instalar suas depenencias, porém ela faria exatamente o proprosito do projeto, uma pena\n",
        "</i>"
      ],
      "metadata": {
        "id": "aX3mJGuiUNyD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Tentativa 2<h1>\n",
        "<h2>NLTK - Natural Language Toolkit<h2>"
      ],
      "metadata": {
        "id": "sIEFxgoFUmmi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.chat.util import Chat, reflections"
      ],
      "metadata": {
        "id": "OFNFGBcgHATZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ela necessita de pares de perguntas e respostas no estilo abaixo\n",
        "```python\n",
        "\n",
        "# this is the talking part\n",
        "pairs = [\n",
        "    ['my name is (.*)', ['hi %1']],\n",
        "    ['(hi|hello|hey|hello|greetings|whats up)', ['Greetings Sir', 'Hello There', 'Welcome back Sir']],\n",
        "    ['(.*)help(.*)', ['How may exactly I help you?', 'I can help you, Sir', 'What do you need help with?']],\n",
        "    ['(.*)your name?', ['My name is TRAVIS', 'Call  me TRAVIS', 'Im TRAVIS, your personal assistant']],\n",
        "    ['(.*)created you?', ['Dr. Teo Azevedo created me!']],\n",
        "]\n",
        "```\n",
        "entào eu criei um algortimo que extrair as mensagens do whatsapp nesse formato, ou seja, as mensagens da pessoa ficam como pergunta e as mensagens q eu mando em seguida como resposta"
      ],
      "metadata": {
        "id": "1uUKjEIJVBDh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_pairs(chat_file_path):\n",
        "    lines = []\n",
        "    # Inicialmente, a última mensagem lida é uma string vazia\n",
        "    last_message = \"\"\n",
        "\n",
        "    with open(chat_file_path, 'r', encoding='utf-8') as f:\n",
        "       for line in f:\n",
        "        # Verifica se a linha possui o caractere \" - \"\n",
        "        if \" - \" in line:\n",
        "            # Divide a linha em duas partes usando o caractere \" - \"\n",
        "            date, sender_message = line.lower().split(\" - \", 1)\n",
        "            # Divide o remetente e a mensagem usando o caractere \": \"\n",
        "            sender, message = sender_message.split(\": \", 1)\n",
        "            # Se a última mensagem não for vazia, adiciona-a à lista\n",
        "            message = message.replace(\"(\", \" \").replace(\")\", \" \")\n",
        "            message = message.replace(\"*\", \" \").replace(\"/\", \" \")\n",
        "            message = message.replace(\"?\", \" \").replace('\"', \" \")\n",
        "\n",
        "\n",
        "            if last_message:\n",
        "                lines.append(last_message)\n",
        "            last_message = [sender, message.strip()]  # Atualiza a última mensagem lida\n",
        "        else:\n",
        "            # Considera que a linha atual é uma continuação da última mensagem lida\n",
        "            last_message[1] += \" \" + line.strip()  # Adiciona a nova linha à última mensagem\n",
        "    # Adiciona a última mensagem lida à lista (caso ela não tenha sido adicionada ainda)\n",
        "    if last_message:\n",
        "        lines.append(last_message)\n",
        "\n",
        "        username = ''\n",
        "        value = ''\n",
        "        pairs = []\n",
        "        pair = []\n",
        "        # iterate through the lines\n",
        "        for index, line in enumerate(lines):\n",
        "            # if it's the first line, store the user\n",
        "            if index == 0:\n",
        "                username = line[0]\n",
        "            # if it's the same user, add the message to the same string\n",
        "            if line[0] == username:\n",
        "                value = value + line[1] + ' '\n",
        "            # if it's another user, append the previous message to the list and create a new string\n",
        "            else:\n",
        "                # append the previous message to the list pair\n",
        "                pair.append(value)\n",
        "                # if there are already two messages, append the list to the pairs list and clear the list pair\n",
        "                if len(pair) == 2:\n",
        "                    pair[1] = pair[1]\n",
        "                    pairs.append(pair.copy())\n",
        "                    pair.clear()\n",
        "                # update the current user and create a new string for the message\n",
        "                username = line[0]\n",
        "                value = line[1] + ' '\n",
        "        # append the last message to the list pair\n",
        "        pair.append(value)\n",
        "        # if there are two messages, append the list to the pairs list\n",
        "        if len(pair) == 2:\n",
        "            pair[1] = pair[1]\n",
        "            pairs.append(pair.copy())\n",
        "    return pairs"
      ],
      "metadata": {
        "id": "9UpoHWjXReg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pairs = extract_pairs('chat.txt')"
      ],
      "metadata": {
        "id": "IGN1q8JnTVSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pairs)"
      ],
      "metadata": {
        "id": "UO0qGF8jTW3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chatbot = Chat(pairs)"
      ],
      "metadata": {
        "id": "C7n9F8wJIphP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chatbot.converse()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 710
        },
        "id": "yd2-CRx2I3ku",
        "outputId": "444868e4-9fe0-4acd-9c76-c25a6fd9b643"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">OI \n",
            "sumiu ein kk era o negocio la de algoritmos \n",
            ">kkkkkkkk \n",
            "None\n",
            ">oi \n",
            "sumiu ein kk era o negocio la de algoritmos \n",
            ">wtf \n",
            "None\n",
            ">como ta \n",
            "None\n",
            ">teste \n",
            "None\n",
            ">linda \n",
            "None\n",
            ">lindo \n",
            "None\n",
            ">eu não\n",
            "None\n",
            ">eu não \n",
            "None\n",
            ">por isso que morre \n",
            "deus livre \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-151-f2c53c5b7167>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mchatbot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nltk/chat/util.py\u001b[0m in \u001b[0;36mconverse\u001b[0;34m(self, quit)\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m                 \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\">\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m             )\n\u001b[0;32m--> 860\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    861\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<i><h3>Tentativa 2 - Funcinou porém não como esperado</h3>\n",
        "Motivo: o modulo Chat da biblioteca nltk na verdade não é uma IA mas sim um algortimo de perguntas e respostas, algo que eu mesmo poderia fazer na mão, ou seja, é necessario uma pergunta EXATA para gerar uma resposta EXATA, nào é o resultado que buscamos\n",
        "</i>"
      ],
      "metadata": {
        "id": "H8JAmig9VqP3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<H1>Tentativa 3<h1>\n",
        "<H2>Fasttext<h2>"
      ],
      "metadata": {
        "id": "vszwqo14WB8J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fasttext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOZXWIbG33fq",
        "outputId": "e2206642-2be9-4c64-e7a3-32d7bfa61e06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[K     |████████████████████████████████| 68 kB 3.3 MB/s \n",
            "\u001b[?25hCollecting pybind11>=2.2\n",
            "  Using cached pybind11-2.10.1-py3-none-any.whl (216 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from fasttext) (57.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from fasttext) (1.21.6)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp38-cp38-linux_x86_64.whl size=3132511 sha256=043f226fd0969b4d4133fd64e558f1a312e645a0e19f20a8329f87bc4431a921\n",
            "  Stored in directory: /root/.cache/pip/wheels/93/61/2a/c54711a91c418ba06ba195b1d78ff24fcaad8592f2a694ac94\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.10.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext\n",
        "\n",
        "# Salve o conjunto de dados de treinamento em um arquivo\n",
        "with open('train_data.txt', 'w') as f:\n",
        "    for question, answer in f:\n",
        "        f.write(f'__label__{answer} {question}\\n')\n",
        "\n",
        "# Crie o modelo de classificação de texto\n",
        "model = fasttext.train_supervised('train_data.txt')\n",
        "\n",
        "# Preveja a resposta para a pergunta de teste\n",
        "predictions = model.predict('oi')\n",
        "print(predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "5esJwE893112",
        "outputId": "640cee63-c178-4bb1-9621-5f4cd6744210"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnsupportedOperation",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnsupportedOperation\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-154-ae89a07cc953>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Salve o conjunto de dados de treinamento em um arquivo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_data.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'__label__{answer} {question}\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnsupportedOperation\u001b[0m: not readable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<i><h3>Tentativa 3 - Funcinou porém não como esperado</h3>\n",
        "Motivo: a fasttext na verdade é uma IA para classificações de mensagens e encontrar palavras diferentes com o mesmo significado, portanto, até da pra criar uma gambiarra criando uma classificação para cada resposta, e ele classifica a pergunta com uma resposta, mas não é o que queremos ainda\n",
        "</i>"
      ],
      "metadata": {
        "id": "kTGxNg5XWPQV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Tentativa 4<h1>\n",
        "<h2>TensorFlow, NLP com modelo baseado em transformadores<h2>"
      ],
      "metadata": {
        "id": "6JKislgXWh3X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<i>\n",
        "Existem vários tipos de modelos de aprendizado de máquina que você pode usar para treinar sua IA de perguntas e respostas. Alguns exemplos incluem modelos de linguagem baseados em transformadores, modelos de linguagem baseados em regiões, e modelos de linguagem baseados em redes neurais.\n",
        "Os modelos baseados em transformadores são uma classe de modelos de aprendizado de máquina que têm sido muito bem-sucedidos em tarefas de processamento de linguagem natural, como tradução de idiomas, resumo de texto e resposta a perguntas. Eles são chamados de \"modelos baseados em transformadores\" porque usam uma camada de transformação para processar o texto de entrada.\n",
        "\n",
        "Os modelos baseados em transformadores usam uma técnica chamada atenção para se concentrar em diferentes partes do texto de entrada enquanto processam a saída. Isso permite que eles capturem relações entre palavras e frases de maneira mais precisa do que modelos mais simples.\n",
        "\n",
        "Os modelos baseados em regiões são uma classe de modelos de aprendizado de máquina que também são amplamente utilizados em tarefas de processamento de linguagem natural. Eles são chamados de \"modelos baseados em regiões\" porque usam uma camada de processamento de regiões para processar o texto de entrada.\n",
        "\n",
        "Os modelos baseados em regiões funcionam dividindo o texto de entrada em regiões pequenas e processando cada uma delas separadamente. Isso permite que eles capturem relações entre palavras e frases de maneira mais precisa do que modelos mais simples.\n",
        "\n",
        "Ambos os modelos baseados em transformadores e modelos baseados em regiões podem ser úteis para capturar a maneira como uma pessoa responde a uma pergunta e imitar esse comportamento. No entanto, os modelos baseados em transformadores tendem a ser mais precisos em tarefas de processamento de linguagem natural, como resposta a perguntas, e podem ser a escolha mais adequada para essa finalidade.\n",
        "\n",
        "Isso se deve ao fato de que os modelos baseados em transformadores usam a técnica de atenção, o que os permite se concentrar em diferentes partes do texto de entrada enquanto processam a saída. Isso os torna mais capazes de capturar relações entre palavras e frases de maneira mais precisa.\n",
        "\n",
        "No entanto, é importante notar que a escolha do modelo ideal dependerá do conjunto de dados de treinamento e da tarefa específica em questão. É sempre uma boa ideia avaliar vários modelos e escolher aquele que apresenta o melhor desempenho.\n",
        "\n",
        "Uma biblioteca comum que pode ser usada para treinar modelos baseados em transformadores para tarefas de processamento de linguagem natural em Python é o TensorFlow. O TensorFlow é uma biblioteca de aprendizado de máquina open-source que fornece uma ampla variedade de ferramentas e funcionalidades para treinar modelos de aprendizado de máquina.\n",
        "\n",
        "Para treinar um modelo baseado em transformadores para tarefas de perguntas e respostas, você pode usar o módulo de linguagem do TensorFlow, que inclui várias ferramentas e funcionalidades específicas para esse tipo de tarefa. Por exemplo, o módulo de linguagem do TensorFlow inclui o TensorFlow Datasets, que é uma coleção de conjuntos de dados pré-processados para tarefas de linguagem, e o TensorFlow Text, que fornece ferramentas para pré-processar texto e criar representações de texto para uso em modelos de linguagem.\n",
        "</i>"
      ],
      "metadata": {
        "id": "EHaqQmTiXFO-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQhA6laW-dQM",
        "outputId": "3cbcb70e-e937-4720-cb3f-f16dca43cba8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 36.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.2)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 43.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (5.1.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data():\n",
        "    # Carregar a matriz de pares de pergunta-resposta\n",
        "    pairs = extract_pairs('chat.txt')\n",
        "\n",
        "    # Separar as perguntas e respostas em listas separadas\n",
        "    questions = [pair[0] for pair in pairs]\n",
        "    answers = [pair[1] for pair in pairs]\n",
        "\n",
        "    return questions, answers"
      ],
      "metadata": {
        "id": "YtzTuCDk_jTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import TFAutoModel, TFAutoModelForSeq2SeqLM, TFAutoModelWithLMHead, AutoTokenizer"
      ],
      "metadata": {
        "id": "6PaVNRY5_p60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_transformer_model():\n",
        "    # Escolha o modelo de transformadores e o tokenizador\n",
        "    transformer_model = TFAutoModel.from_pretrained(\"bert-base-uncased\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "    # Crie o modelo de sequência para sequência com cabeça de linguagem\n",
        "    model = TFAutoModelWithLMHead.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "xmDwxDQ9_srT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract_pairs('chat.txt')\n",
        "questions, answers = load_data()"
      ],
      "metadata": {
        "id": "2SSw0Qq7_6w0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criar o modelo de transformadores\n",
        "model = create_transformer_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZIsdXqZ_8K0",
        "outputId": "c5493b8d-927f-41d8-c2df-5bd030352360"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFBertForMaskedLM.\n",
            "\n",
            "All the layers of TFBertForMaskedLM were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compilar o modelo com uma função de perda e otimizador\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "metadata": {
        "id": "TW7CFvdpADJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criar o tokenizador\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "id": "TENJigtHA38l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converta as perguntas e respostas em sequências de números inteiros\n",
        "questions = [tokenizer.encode(q, add_special_tokens=True) for q in questions]\n",
        "answers = [tokenizer.encode(a, add_special_tokens=True) for a in answers]\n",
        "\n",
        "# Preencha as sequências de números inteiros para que elas tenham o mesmo comprimento\n",
        "MAX_LENGTH = max(len(q) for q in questions + answers)\n",
        "questions = [q + [0] * (MAX_LENGTH - len(q)) for q in questions]\n",
        "answers = [a + [0] * (MAX_LENGTH - len(a)) for a in answers]"
      ],
      "metadata": {
        "id": "YXcfpG0nA8zd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criar um dataset a partir das perguntas e respostas\n",
        "dataset = tf.data.Dataset.from_tensor_slices((questions, answers))\n",
        "\n",
        "# Empacotar os exemplos em batches\n",
        "batch_size = 32\n",
        "dataset = dataset.batch(batch_size)\n"
      ],
      "metadata": {
        "id": "VrrXuBvFBR8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Treinar o modelo com os dados de perguntas e respostas\n",
        "model.fit(questions, answers, epochs=10)\n",
        "\n",
        "# Salvar o modelo treinado para uso posterior\n",
        "model.save('qa_model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 832
        },
        "id": "O_b5AHQu_yT3",
        "outputId": "570dbac8-ee76-4e54-afcc-0775f0f84eb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-184-f293fc2d3332>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Treinar o modelo com os dados de perguntas e respostas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Salvar o modelo treinado para uso posterior\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'qa_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/modeling_tf_utils.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1554\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompiled_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregularization_losses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1556\u001b[0m         \u001b[0;31m# Run backwards pass.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.8/dist-packages/transformers/modeling_tf_utils.py\", line 1554, in train_step\n        loss = self.compiled_loss(y, y_pred, sample_weight, regularization_losses=self.losses)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/losses.py\", line 139, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/losses.py\", line 243, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/losses.py\", line 1787, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/usr/local/lib/python3.8/dist-packages/keras/backend.py\", line 5119, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 393) and (None, 393, 30522) are incompatible\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Tentativa 4 - Falha por falta de conhecimento<h3>\n",
        "<i>Motivo: como deu pra perceber, esse codigo que tentei fazer é bastante complexo, aborda diversos temas teoricos de Ciencia da Computação que ainda nào aprendi, teve um momento que a biblioteca precisava que eu convertesse texto pra numeros, wtf, depois eu volto aqui e tento entender melhor a biblioteca ou então eu tento do 0 com uma outra mais simples, com algo mais pronto.</i>\n"
      ],
      "metadata": {
        "id": "Wb7hAeeJXT6B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Tentativa 5<h1>\n",
        "<h2>spaCy<h2>"
      ],
      "metadata": {
        "id": "aTP2n03Yg5sU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSjCFmAnh6tD",
        "outputId": "b34a7881-5e3e-4eec-e63d-43f42facda2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.8/dist-packages (3.4.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (8.1.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.21.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.4.5)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (4.64.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.10.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy) (2.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import random\n",
        "\n",
        "# Carregue os dados de treinamento\n",
        "with open(\"train_data.txt\", \"r\") as f:\n",
        "    train_data = f.read().splitlines()\n",
        "\n",
        "# Crie o modelo de rede neural de linguagem utilizando o módulo de treinamento de redes neurais da biblioteca spaCy\n",
        "nlp = spacy.blank(\"en\")\n",
        "nlp.add_pipe(\"textcat\")\n",
        "nlp.add_label(\"pergunta\")\n",
        "nlp.add_label(\"resposta\")\n",
        "\n",
        "# Inicialize os pesos do modelo com valores aleatórios\n",
        "optimizer = nlp.begin_training()\n",
        "\n",
        "# Defina o tamanho do lote e o número de épocas\n",
        "batch_size = 8\n",
        "n_epochs = 5\n",
        "\n",
        "# Itera pelo número de épocas\n",
        "for epoch in range(n_epochs):\n",
        "    # Misture os dados de treinamento aleatoriamente\n",
        "    random.shuffle(train_data)\n",
        "    \n",
        "    # Divide os dados de treinamento em lotes\n",
        "    batches = [train_data[i:i+batch_size] for i in range(0, len(train_data), batch_size)]\n",
        "    \n",
        "    # Itera pelos lotes\n",
        "    for batch in batches:\n",
        "        # Cria uma lista vazia para armazenar os erros\n",
        "        errors = []\n",
        "        \n",
        "        # Itera pelas perguntas e respostas no lote\n",
        "        for pergunta, resposta in batch:\n",
        "            # Crie um documento com a pergunta e adicione a label \"pergunta\"\n",
        "            doc = nlp.make_doc(pergunta)\n",
        "            doc.is_parsed = True\n",
        "            doc.ents = ()\n",
        "            doc.user_data[\"labels\"] = (\"pergunta\",)\n",
        "            \n",
        "            # Adicione o documento à lista de erros\n",
        "            errors.extend(nlp.update([doc], [resposta]))\n",
        "        \n",
        "        # Atualize os pesos do modelo utilizando o otimizador\n",
        "        optimizer.update(errors)\n",
        "\n",
        "# Salve o modelo treinado em um arquivo\n",
        "nlp.to_disk(\"model.spacy\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "ZEK8TPqahA1j",
        "outputId": "74ceb7a4-953e-4436-c312-4f3cd57b8a08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-0b900cf32a12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"en\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"textcat\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pergunta\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"resposta\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'English' object has no attribute 'add_label'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Tentativa 6</h1>"
      ],
      "metadata": {
        "id": "-SFMASHkmXTA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "matriz = pairs"
      ],
      "metadata": {
        "id": "yl3K6EXfrXGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "json_data = {\n",
        "    \"intents\": []\n",
        "}\n",
        "i = 0\n",
        "for pergunta, resposta in matriz:\n",
        "    intent = {\n",
        "        \"tag\": f\"{i}\",\n",
        "        \"patterns\": [pergunta],\n",
        "        \"responses\": [resposta]\n",
        "    }\n",
        "    json_data[\"intents\"].append(intent)\n",
        "    i = i+1\n",
        "\n",
        "json_string = json.dumps(json_data, ensure_ascii=False, indent=4)\n",
        "\n",
        "with open(\"intents.json\", \"w\") as f:\n",
        "    f.write(json_string)\n"
      ],
      "metadata": {
        "id": "sfM_Wx2HrB02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tflearn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ah4a45mKkXdo",
        "outputId": "a6c0c5d6-b535-41ff-dce1-f5692056110c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tflearn\n",
            "  Downloading tflearn-0.5.0.tar.gz (107 kB)\n",
            "\u001b[K     |████████████████████████████████| 107 kB 15.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from tflearn) (1.21.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from tflearn) (1.15.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (from tflearn) (7.1.2)\n",
            "Building wheels for collected packages: tflearn\n",
            "  Building wheel for tflearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tflearn: filename=tflearn-0.5.0-py3-none-any.whl size=127299 sha256=47d086f2e5b4f5bec4db512854014e42cb4aa22798f35af782b4b69bb60bdb8d\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/9b/15/cb1e6b279c14ed897530d15cfd7da8e3df8a947e593f5cfe59\n",
            "Successfully built tflearn\n",
            "Installing collected packages: tflearn\n",
            "Successfully installed tflearn-0.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Used in Tensorflow Model\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tflearn\n",
        "import random\n",
        "\n",
        "#Usde to for Contextualisation and Other NLP Tasks.\n",
        "import nltk\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "stemmer = LancasterStemmer()\n",
        "\n",
        "#Other\n",
        "import json\n",
        "import pickle\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wT4i4VNxj7YZ",
        "outputId": "02b71b5b-71cd-4b4a-d028-e29499edf885"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Processing the Intents.....\")\n",
        "with open('intents.json') as json_data:\n",
        "    intents = json.load(json_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZuzxA7nj99B",
        "outputId": "119a0f2a-eda1-4ed4-fb1d-84092add612c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing the Intents.....\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKqGrVfCkyt5",
        "outputId": "78a063f5-6933-4ed4-978d-2b3e4877a8c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = []\n",
        "classes = []\n",
        "documents = []\n",
        "ignore_words = ['?']\n",
        "print(\"Looping through the Intents to Convert them to words, classes, documents and ignore_words.......\")\n",
        "for intent in intents['intents']:\n",
        "    for pattern in intent['patterns']:\n",
        "        # tokenize each word in the sentence\n",
        "        w = nltk.word_tokenize(pattern)\n",
        "        # add to our words list\n",
        "        words.extend(w)\n",
        "        # add to documents in our corpus\n",
        "        documents.append((w, intent['tag']))\n",
        "        # add to our classes list\n",
        "        if intent['tag'] not in classes:\n",
        "            classes.append(intent['tag'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CaNjn9g3j_6w",
        "outputId": "61856527-079c-4134-a4dd-d43071726c3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looping through the Intents to Convert them to words, classes, documents and ignore_words.......\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Stemming, Lowering and Removing Duplicates.......\")\n",
        "words = [stemmer.stem(w.lower()) for w in words if w not in ignore_words]\n",
        "words = sorted(list(set(words)))\n",
        "\n",
        "# remove duplicates\n",
        "classes = sorted(list(set(classes)))\n",
        "\n",
        "print (len(documents), \"documents\")\n",
        "print (len(classes), \"classes\", classes)\n",
        "print (len(words), \"unique stemmed words\", words)"
      ],
      "metadata": {
        "id": "rdANSmWEkBfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Creating the Data for our Model.....\")\n",
        "training = []\n",
        "output = []\n",
        "print(\"Creating an List (Empty) for Output.....\")\n",
        "output_empty = [0] * len(classes)\n",
        "\n",
        "print(\"Creating Traning Set, Bag of Words for our Model....\")\n",
        "for doc in documents:\n",
        "    # initialize our bag of words\n",
        "    bag = []\n",
        "    # list of tokenized words for the pattern\n",
        "    pattern_words = doc[0]\n",
        "    # stem each word\n",
        "    pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n",
        "    # create our bag of words array\n",
        "    for w in words:\n",
        "        bag.append(1) if w in pattern_words else bag.append(0)\n",
        "\n",
        "    # output is a '0' for each tag and '1' for current tag\n",
        "    output_row = list(output_empty)\n",
        "    output_row[classes.index(doc[1])] = 1\n",
        "\n",
        "    training.append([bag, output_row])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTsrGGlzkD0I",
        "outputId": "bafbf87d-132f-47c6-c8d7-901b970dcb08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating the Data for our Model.....\n",
            "Creating an List (Empty) for Output.....\n",
            "Creating Traning Set, Bag of Words for our Model....\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shuffling Randomly and Converting into Numpy Array for Faster Processing......\")\n",
        "random.shuffle(training)\n",
        "training = np.array(training)\n",
        "\n",
        "print(\"Creating Train and Test Lists.....\")\n",
        "train_x = list(training[:,0])\n",
        "train_y = list(training[:,1])\n",
        "print(\"Building Neural Network for Out Chatbot to be Contextual....\")\n",
        "print(\"Resetting graph data....\")\n",
        "tf.compat.v1.reset_default_graph()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EB4W93vHkFzm",
        "outputId": "29beadc4-3b60-4a10-8981-c409d09a0658"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shuffling Randomly and Converting into Numpy Array for Faster Processing......\n",
            "Creating Train and Test Lists.....\n",
            "Building Neural Network for Out Chatbot to be Contextual....\n",
            "Resetting graph data....\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net = tflearn.input_data(shape=[None, len(train_x[0])])\n",
        "net = tflearn.fully_connected(net, 8)\n",
        "net = tflearn.fully_connected(net, 8)\n",
        "net = tflearn.fully_connected(net, len(train_y[0]), activation='softmax')\n",
        "net = tflearn.regression(net)\n",
        "print(\"Training....\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qm5KGU3MkHeZ",
        "outputId": "31846293-0aa3-457d-a218-8234242cf3d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tflearn/initializations.py:164: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training....\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = tflearn.DNN(net, tensorboard_dir='tflearn_logs')"
      ],
      "metadata": {
        "id": "sFy6W0A2kItM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training the Model.......\")\n",
        "model.fit(train_x, train_y, n_epoch=1000, batch_size=8, show_metric=True)\n",
        "print(\"Saving the Model.......\")\n",
        "model.save('model.tflearn')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nE_WCa0kKWw",
        "outputId": "5d2753d6-102a-46b5-d1d6-fb23a70041ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Step: 139999  | total loss: \u001b[1m\u001b[32m0.20425\u001b[0m\u001b[0m | time: 0.940s\n",
            "| Adam | epoch: 1000 | loss: 0.20425 - acc: 0.9337 -- iter: 1112/1120\n",
            "Training Step: 140000  | total loss: \u001b[1m\u001b[32m0.27494\u001b[0m\u001b[0m | time: 0.949s\n",
            "| Adam | epoch: 1000 | loss: 0.27494 - acc: 0.9029 -- iter: 1120/1120\n",
            "--\n",
            "Saving the Model.......\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('model.tflearn')"
      ],
      "metadata": {
        "id": "ycZHVUWm_3nR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Pickle is also Saved..........\")\n",
        "pickle.dump( {'words':words, 'classes':classes, 'train_x':train_x, 'train_y':train_y}, open( \"training_data\", \"wb\" ) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9SY9L2QkNC8",
        "outputId": "374e30b0-a063-4e3d-b3b8-2732ad80be2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pickle is also Saved..........\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading Pickle.....\")\n",
        "data = pickle.load( open( \"training_data\", \"rb\" ) )\n",
        "words = data['words']\n",
        "classes = data['classes']\n",
        "train_x = data['train_x']\n",
        "train_y = data['train_y']\n",
        "\n",
        "\n",
        "with open('intents.json') as json_data:\n",
        "    intents = json.load(json_data)\n",
        "    \n",
        "print(\"Loading the Model......\")\n",
        "# load our saved model\n",
        "model.load('./model.tflearn')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVmaT5lBkOdJ",
        "outputId": "581bb556-ebf9-49ed-e7c0-5cfdc08efcfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Pickle.....\n",
            "Loading the Model......\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_up_sentence(sentence):\n",
        "    # It Tokenize or Break it into the constituents parts of Sentense.\n",
        "    sentence_words = nltk.word_tokenize(sentence)\n",
        "    # Stemming means to find the root of the word.\n",
        "    sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
        "    return sentence_words\n",
        "\n",
        "# Return the Array of Bag of Words: True or False and 0 or 1 for each word of bag that exists in the Sentence\n",
        "def bow(sentence, words, show_details=False):\n",
        "    sentence_words = clean_up_sentence(sentence)\n",
        "    bag = [0]*len(words)\n",
        "    for s in sentence_words:\n",
        "        for i,w in enumerate(words):\n",
        "            if w == s:\n",
        "                bag[i] = 1\n",
        "                if show_details:\n",
        "                    print (\"found in bag: %s\" % w)\n",
        "    return(np.array(bag))\n",
        "\n",
        "ERROR_THRESHOLD = 0.25\n",
        "print(\"ERROR_THRESHOLD = 0.25\")\n",
        "\n",
        "def classify(sentence):\n",
        "    # Prediction or To Get the Posibility or Probability from the Model\n",
        "    results = model.predict([bow(sentence, words)])[0]\n",
        "    # Exclude those results which are Below Threshold\n",
        "    results = [[i,r] for i,r in enumerate(results) if r>ERROR_THRESHOLD]\n",
        "    # Sorting is Done because heigher Confidence Answer comes first.\n",
        "    results.sort(key=lambda x: x[1], reverse=True)\n",
        "    return_list = []\n",
        "    for r in results:\n",
        "        return_list.append((classes[r[0]], r[1])) #Tuppl -> Intent and Probability\n",
        "    return return_list\n",
        "\n",
        "def response(sentence, userID='123', show_details=False):\n",
        "    results = classify(sentence)\n",
        "    # That Means if Classification is Done then Find the Matching Tag.\n",
        "    if results:\n",
        "        # Long Loop to get the Result.\n",
        "        while results:\n",
        "            for i in intents['intents']:\n",
        "                # Tag Finding\n",
        "                if i['tag'] == results[0][0]:\n",
        "                    # Random Response from High Order Probabilities\n",
        "                    return print(random.choice(i['responses']))\n",
        "\n",
        "            results.pop(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqLSee91kQcg",
        "outputId": "b0f423aa-b161-4a91-cdf2-8319f282d55d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR_THRESHOLD = 0.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    input_data = input(\"You- \")\n",
        "    answer = response(input_data)\n",
        "    answer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 797
        },
        "id": "1KfE8wCAkUcA",
        "outputId": "f675eb2a-a2b0-4338-ae6f-4c37b3717cef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You- qual teu insta?\n",
            "💔 \n",
            "You- wtf\n",
            "You- me passa seu insta\n",
            "mentira eu n sei 💔 é muito no automático msm \n",
            "You- como ta?\n",
            "kkkkkkkkkkkkk \n",
            "You- Como ta?\n",
            "kkkkkkkkkkkkk \n",
            "You- Como voce ta?\n",
            "kkkkkkkkkkkkk \n",
            "You- sexo agora\n",
            "o de detectar cáncer tem galera de odonto tbm mto bacaninha kkkk \n",
            "You- atacante\n",
            "mas não dá pra negar q já fiz coisas \n",
            "You- micael\n",
            "mas eu quero ver \n",
            "You- oi\n",
            "You- como ta\n",
            "kkkkkkkkkkkkk \n",
            "You- oi \n",
            "You- oi\n",
            "You- como ta?\n",
            "kkkkkkkkkkkkk \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-b154cd173fb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0minput_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You- \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0manswer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m             )\n\u001b[0;32m--> 860\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    861\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Y8NBRSGsLvCX"
      }
    }
  ]
}
